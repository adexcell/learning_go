# Web Crawler

Консольная утилита на Go для рекурсивного скачивания веб-сайтов (аналог wget). Программа скачивает страницы, сохраняет их локальную структуру и переписывает ссылки для корректного офлайн-просмотра.

## Возможности

*   **Рекурсивный обход**: Скачивание страниц, переходя по ссылкам на заданную глубину.
*   **Конкурентность**: Параллельная загрузка страниц для высокой скорости работы.
*   **Офлайн-просмотр**: Автоматическое переписывание абсолютных ссылок (`href`, `src`) на относительные локальные пути.
*   **Ограничение по домену**: Скачиваются только ресурсы в пределах домена стартового URL.
*   **Сохранение ресурсов**: Поддержка скачивания изображений, скриптов и стилей (если они указаны в тегах `img`, `script`, `link`).

## Требования

*   Go 1.18 или выше

## Установка

1.  Склонируйте репозиторий.
2.  Загрузите зависимости:

```bash
go mod download
```

## Использование

Запустите утилиту, указав URL сайта:

```bash
go run main.go [флаги] <url>
```

### Флаги

*   `-l <int>`: Глубина рекурсии (по умолчанию `0` — скачивается только стартовая страница).
*   `-c <int>`: Максимальное количество конкурентных потоков (по умолчанию `10`).

### Примеры

**Скачать одну страницу:**

```bash
go run main.go https://example.com
```

**Скачать сайт на глубину 2 с использованием 20 потоков:**

```bash
go run main.go -l 2 -c 20 https://golang.org
```

## Результат работы

Скачанные файлы сохраняются в директорию `downloads` в корне проекта. Структура папок повторяет структуру URL сайта.
